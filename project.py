# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M6XdMdJmd_XWgkAqFA_5zAhmfcx5Er9u

# **Diabetes prediction with explainable AI (BINARY CLASSIFICATION)**
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_excel("diabetes.csv.xlsx")
display(df.head())

# Cell 1 — imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import shap
import joblib  # for saving scaler

# For nicer plots
sns.set(style="whitegrid")

# Cell 3 — quick EDA
print(df.info())
print(df.describe())

import seaborn as sns
import matplotlib.pyplot as plt

# Class distribution
sns.countplot(x='Outcome', data=df)
plt.title('Diabetic vs Non-Diabetic Count')
plt.show()

# Correlation heatmap
plt.figure(figsize=(8,6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# Cell 4 — Replace zeros with NaN for physiologically-impossible zeros
cols_with_zero = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']
df[cols_with_zero] = df[cols_with_zero].replace(0, np.nan)

# Show missing count
print(df.isna().sum())

# Impute missing values with median (robust)
df.fillna(df.median(), inplace=True)
print("Missing after impute:", df.isna().sum().sum())

# Cell 5 — Feature/Label split and scaling
X = df.drop('Outcome', axis=1)
y = df['Outcome']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Save scaler for later demo predictions
joblib.dump(scaler, 'scaler.save')

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)
print("Train/Test sizes:", X_train.shape, X_test.shape)

# Cell 6 — build the ANN
model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.25),
    Dense(16, activation='relu'),
    Dropout(0.15),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Cell 7 — training with early stopping
es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=200,
    batch_size=16,
    callbacks=[es],
    verbose=2
)
# Save model
model.save('diabetes_ann.h5')

# Cell 8 — plot training curves
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend(); plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend(); plt.title('Loss')
plt.show()

# Cell 9 — evaluation
y_prob = model.predict(X_test).ravel()
y_pred = (y_prob >= 0.5).astype(int)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix')
plt.show()

# ROC AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, y_prob):.3f}')
plt.plot([0,1],[0,1],'--', color='gray')
plt.legend(); plt.title('ROC Curve'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.show()

# Cell 10 — SHAP explainability (DeepExplainer / KernelExplainer)
# Note: SHAP can be slow with large models. For small tabular model, use KernelExplainer or TreeExplainer is for tree models.
# For Keras models with tabular data, KernelExplainer works (approx). We'll sample background to speed it up.

# 10.1 prepare a small background sample (use training set)
X_train_sample = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]

# 10.2 define a wrapper prediction function that returns probability
def predict_proba_for_shap(x):
    return model.predict(x).ravel()

# 10.3 create explainer (KernelExplainer)
explainer = shap.KernelExplainer(predict_proba_for_shap, X_train_sample, link="logit")

# 10.4 compute SHAP values for a small subset of test (to keep runtime small)
X_test_sample = X_test[np.random.choice(X_test.shape[0], 50, replace=False)]
shap_values = explainer.shap_values(X_test_sample, nsamples=100)

# 10.5 global summary plot
feature_names = list(X.columns)
shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, plot_type="bar", show=True)

shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, show=True)

# Cell 11 — per-sample explanation (force plot) for one test sample
# Pick one example from X_test (unscaled original feature order)
idx = 10
sample = X_test[idx].reshape(1, -1)
sample_shap = explainer.shap_values(sample, nsamples=200)

# Force plot (visual interactive).
shap.initjs()
shap.force_plot(explainer.expected_value, sample_shap, sample, feature_names=feature_names, matplotlib=True)

# Cell 12 — mapping back features and demo prediction for a new raw patient sample
# Save scaler and model already done
# Example raw input row (values in the original units)
raw_sample = np.array([[2, 120, 70, 25, 100, 28.0, 0.5, 35]])  # modify values
sc = joblib.load('scaler.save')
raw_scaled = sc.transform(raw_sample)
prob = model.predict(raw_scaled)[0][0]
print("Predicted probability of diabetes:", prob)
print("Predicted class:", "Diabetic" if prob>=0.5 else "Non-Diabetic")

